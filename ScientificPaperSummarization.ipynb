{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PyPDF2\n",
    "import docx2txt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Function to read the document from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDoc():\n",
    "    name = input('Please input a file name: ') \n",
    "    print('You have asked for the document {}'.format(name))\n",
    "\n",
    "    if name.lower().endswith('.txt'):\n",
    "        choice = 1\n",
    "    elif name.lower().endswith('.pdf'):\n",
    "        choice = 2\n",
    "    else:\n",
    "        choice = 3\n",
    "    print(choice)\n",
    "        \n",
    "    if choice == 1:\n",
    "        f = open(name, 'r')\n",
    "        document = f.read()\n",
    "        f.close()\n",
    "            \n",
    "    elif choice == 2:\n",
    "        pdfFileObj = open(name, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pageObj = pdfReader.getPage(0)\n",
    "        document = pageObj.extractText()\n",
    "        pdfFileObj.close()\n",
    "    \n",
    "    else:\n",
    "        print('Failed to load a valid file')\n",
    "        print('Returning an empty string')\n",
    "        document = ''\n",
    "    \n",
    "    print(type(document))\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to tokenize the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    doc_tokenizer = PunktSentenceTokenizer()\n",
    "    \n",
    "    sentences_list = doc_tokenizer.tokenize(document)\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Read the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = readDoc()\n",
    "print('The length of the file is:', end=' ')\n",
    "print(len(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate a list of sentences in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the list in Bytes is: 2048\n",
      "The size of the item 0 in Bytes is: 158\n"
     ]
    }
   ],
   "source": [
    "sentences_list = tokenize(document)\n",
    "\n",
    "print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))\n",
    "\n",
    "print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the list \"sentences\" is: 249\n"
     ]
    }
   ],
   "source": [
    "print('The size of the list \"sentences\" is: {}'.format(len(sentences_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract-Text Summarization is the process of obtaining \n",
      "salient information from an authentic text document.\n",
      "In this \n",
      "technique, the extracted information is achieved as a summarized \n",
      "report and conferred as a concise summary to the user.\n",
      "It is very \n",
      "crucial for humans to understand and to describe the content \n",
      "of the text.\n",
      "Text Summarization techniques are classified into \n",
      "abstractive and extractive summarization.\n",
      "The extractive summa\u0002rization technique focuses on choosing how paragraphs,important \n",
      "sentences, etc produces the original documents in precise form.\n",
      "The implication of sentences is determined based on linguistic \n",
      "and statistical features.\n",
      "In this work, a comprehensive review \n",
      "of extractive text summarization process methods has been \n",
      "ascertained.\n",
      "In this paper, the various techniques, populous \n",
      "benchmarking datasets and challenges of extractive summariza\u0002tion have been reviewed.\n",
      "This paper interprets extractive text \n",
      "summarization methods with a less redundant summary, highly \n",
      "adhesive, coherent and depth information.\n",
      "Index Terms-Text Summarization, Unsupervised Learning, \n",
      "Supervised Learning, Sentence Fusion, Extraction Scheme, Sen\u0002tence Revision, Extractive Summary \n",
      "I. INTRODUCTION \n",
      "In a recent advance, the significance of text summarization \n",
      "[1] accomplishes more attention due to data inundation on \n",
      "the web.\n",
      "Hence this information overwhelms yields in the \n",
      "big requirement for more reliable and capable progressive text \n",
      "summarizers.\n",
      "Text Summarization gains its importance due to \n",
      "its various types of applications just like the summaries of \n",
      "books, digest- (summary of stories), the stock market, news, \n",
      "Highlights- (meeting, event, sport), Abstract of scientific pa\u0002pers, newspaper articles, magazine etc.\n",
      "Due to its tremendous \n",
      "growth, many finest universities like Faculty of Informatics \n",
      "- Masaryk University, Czech Republic, Concordia University, \n",
      "Montreal, Canada- Semantic Software Lab, IHR Nexus Lab \n",
      "at Arizona State University, Arizona, USA and finally Lab of \n",
      "Topic Maps-Leipzig University, Germany has been persistently \n",
      "working on its rapid enhancements.\n",
      "Text summarization has grown into a crucial and appropriate \n",
      "engine for supporting and illustrate text content in the latest \n",
      "speedy emergent information age.\n",
      "It's far very complex for \n",
      "humans to physically summarize oversized documents of \n",
      "text.\n",
      "There is a wealth of textual content available on the \n",
      "internet.\n",
      "But, usually, the internet contribute more data than \n",
      "is desired.\n",
      "Therefore, a twin problem is detected: Seeking for \n",
      "appropriate documents through an awe-inspiring number of \n",
      "reports offered, and fascinating a high volume of important \n",
      "information.\n",
      "The objective of automatic text summarization is \n",
      "to condense the origin text into a precise version preserves its report content and global denotation.\n",
      "The main advantage \n",
      "of a text summarization is reading time of the user can be \n",
      "reduced.\n",
      "A marvelous text summary system should reproduce \n",
      "the assorted theme of the document even as keeping repetition \n",
      "to a minimum.\n",
      "Text Summarization methods are publicly \n",
      "restricted into abstractive and extractive summarization.\n",
      "An extractive summarization technique consists of selecting \n",
      "vital sentences, paragraphs, etc, from the original manuscript \n",
      "and concatenating them into a shorter form.\n",
      "The significance \n",
      "of sentences is strongly based on statistical and linguistic \n",
      "features of sentences.\n",
      "This paper generally summarizes the \n",
      "extensive methodologies fitted, issues launch, exploration and \n",
      "future directions in text summarization.\n",
      "This paper [1] is \n",
      "organized as follows.\n",
      "Section 2 depicts about the features for \n",
      "extractive text summarization, Section 3 describes extractive \n",
      "text summarization methods, Section 4 illustrate inferences \n",
      "made, Section 5 represent challenges and future research \n",
      "directions, Section 6 detail about evaluation metrics and the \n",
      "final sketch is the conclusion.\n",
      "II.\n",
      "FEATURES FOR EXTRACTIVE TEXT \n",
      "SUMMARIZATION \n",
      "Earlier techniques involve assigning a score to sentences \n",
      "based on a countenance that are predefined based on the \n",
      "methodology applied.\n",
      "Both word level and sentence level \n",
      "features are employed in text summarization literature.\n",
      "Certain \n",
      "features discussed are [2] [3] [4]used to exclusive sentences \n",
      "to be included in the summary are: \n",
      "1.\n",
      "WORD LEVEL FEATURES \n",
      "1.1 Content Word feature \n",
      "Keywords are essential in identifying the importance of the \n",
      "sentence.\n",
      "The sentence that consists of main keywords is most \n",
      "likely included in the final summary.\n",
      "The content (keyword) \n",
      "words are words that are nouns, verbs, adjectives and adverbs \n",
      "that are commonly determined based on tf x idf measure.\n",
      "1.2 Title Word feature \n",
      "The sentences in the original document which consists of \n",
      "words mentioned in the title have greater chances to contribute \n",
      "to the final summary since they serve as indicators of the theme \n",
      "of the document.\n",
      "1.3 Cue phrase feature \n",
      "Cue phrases are words and phrases that indicate the struc\u0002ture of the document flow and it is used as a feature in \n",
      "sentence selection.\n",
      "The sentence that contains cue phrases \n",
      "(e.g.\n",
      "\"denouement\", \"because\", \"this information\", \"summary\", \n",
      "\"develop\", \"desire\" etc.)\n",
      "are mostly to be included in the final \n",
      "summary.\n",
      "1.4 Biased word feature \n",
      "The sentences that consist of biased words are more likely \n",
      "important.\n",
      "The biased words are a list of the predefined set \n",
      "of words that may be domain specific.\n",
      "They are relatively \n",
      "important words that describe the theme of the document.\n",
      "1.5 Upper case word feature \n",
      "The words which are in uppercase such as \"UNICEF\" are \n",
      "considered to be important words and those sentences that \n",
      "consist of these words are termed important in the context of \n",
      "sentence selection for the final summary.\n",
      "2.\n",
      "SENTENCE LEVEL FEATURES \n",
      "2.1 Sentence location feature \n",
      "The sentences that occur in the beginning and the conclusion \n",
      "part of the document are most likely important since most \n",
      "documents are hierarchically structured with important infor\u0002mation in the beginning and the end of the paragraphs.\n",
      "2.2 Sentence length feature \n",
      "The sentence length plays an important role in identifying \n",
      "key sentences.\n",
      "Shorter texts do not convey essential informa\u0002tion and very long sentences also need not be included in the \n",
      "summary.\n",
      "The normalized length of the sentence is calculated \n",
      "as the ratio between a number of words in the sentence to the \n",
      "number of words in the longest sentence in the document.\n",
      "2.3 Paragraph location feature \n",
      "Similar to sentence location, paragraph location also plays \n",
      "a crucial role in selecting key sentences.\n",
      "A Higher score is \n",
      "assigned to the paragraph in the peripheral sections (beginning \n",
      "and end paragraphs of the document).\n",
      "2.4 Sentence-to-Sentence Cohesion \n",
      "The cohesion between sentences for every sentence(s), the \n",
      "similarity between s and alternative sentences are calculated \n",
      "which are summed up and coarse value of the aspect is \n",
      "obtained for s. The feature values are normalized between \n",
      "[0, 1] where value closer to 1.0 indicates a higher degree of \n",
      "cohesion between sentences.\n",
      "III.\n",
      "EXTRACTIVE TEXT SUMMARIZATION \n",
      "METHODS \n",
      "Extractive Text Summarization methods can be broadly \n",
      "classified as Unsupervised Learning and Supervised learning \n",
      "methods.\n",
      "Recent works rely on Unsupervised Learning meth\u0002ods for text summarization.\n",
      "A. UNSUPERVISED LEARNING METHODS \n",
      "In this section, unsupervised techniques for sentence extrac\u0002tion task is discussed.\n",
      "The unsupervised approaches do not \n",
      "need human summaries (user input) in deciding the important \n",
      "features of the document, it requires the most sophisticated \n",
      "algorithm to provide compensation for the lack of human \n",
      "knowledge.\n",
      "Unsupervised summaries provide a higher level \n",
      "of automation compared to supervised model and are more \n",
      "suitable for processing Big Data.\n",
      "Unsupervised learning mod\u0002els have proved successful in text summarization task.\n",
      "Fig.\n",
      "2.\n",
      "Overview of Unsupervised Learning Methods \n",
      "1.\n",
      "Graph based approach \n",
      "Graph-based models are extensively used in document sum\u0002marization since graphs can efficiently represent the document \n",
      "structure.\n",
      "Extractive text summarization using external knowl\u0002edge from Wikipedia incorporating bipartite graph framework \n",
      "[4 ]has been used.\n",
      "They have proposed an iterative ranking \n",
      "algorithm (variation of HITS algorithm [5]) which is efficient \n",
      "in selecting important sentences and also ensures coherency \n",
      "in the final summary.\n",
      "The uniqueness of this paper is that \n",
      "it combines both graph based and concept based approach \n",
      "towards summarization task.\n",
      "Another graph based approach \n",
      "LexRank [6], where the salience of the sentence is determined \n",
      "by the concept of Eigen vector centrality.\n",
      "The sentences in the \n",
      "document are represented as a graph and the edges between \n",
      "the sentences represents weighted cosine similarity values.\n",
      "The \n",
      "sentences are clustered into groups based on their similarity \n",
      "measures and then the sentences are ranked based on their \n",
      "LexRank scores similar to PageRank algorithm [7]except that \n",
      "the similarity graph is undirected in LexRank method.\n",
      "The \n",
      "method outperforms earlier versions of lead and centroid based \n",
      "approaches.\n",
      "The performance of the system is evaluated with \n",
      "DUC dataset [8].\n",
      "2.\n",
      "Fuzzy logic based approach \n",
      "The fuzzy logic approach mainly contains four components: \n",
      "defuzzifier, fuzzifier, fuzzy knowledge base and inference en\u0002gine.\n",
      "The textual characteristics input of Fuzzy logic approach \n",
      "are sentenced length, sentence similarity etc which is later \n",
      "given to the fuzzy system [9] [10].\n",
      "Ladda Suanmali et al [11] proposed fuzzy logic approach \n",
      "is used for automatic text summarization which is the initial \n",
      "step , the text document is pre-processed followed by feature \n",
      "extraction(Title features, Sentence length, Sentence position, \n",
      "Sentence-sentence similarity, term weight, Proper noun and \n",
      "Numerical data.\n",
      "The summary is generated by ordering the \n",
      "ranked sentences in the order they occur in the original \n",
      "document to maintain coherency.\n",
      "The proposed method shows \n",
      "improvement in the quality of summarization but issues such \n",
      "as dangling anaphora are not handled.\n",
      "3.\n",
      "Concept-based approach \n",
      "In concept-based approach, the concepts are extracted from \n",
      "a piece of text from external knowledge base such HowNet \n",
      "[l2]and Wikipedia [4].\n",
      "In the methodology proposed [12], the \n",
      "importance of sentences is calculated based on the concepts \n",
      "retrieved from HowNet instead of words.\n",
      "A conceptual vector \n",
      "model is built to obtain a rough summarization and similarity \n",
      "measures are calculated between the sentences to reduce \n",
      "redundancy in the final summary.\n",
      "A good summarizer fo\u0002cuses on higher coverage and lower redundancy.\n",
      "Ramanatha et al [4] proposed a Wikipedia-based summarization which \n",
      "utilizes graph structure to produce summaries.\n",
      "The method \n",
      "uses Wikipedia to obtain concept for each sentence and \n",
      "builds a sentence-concept bipartite graph as already mentioned \n",
      "in Graph-based summarization.\n",
      "The basic steps in concept \n",
      "based summarization are: i) Retrieve concepts of a text from external knowledge base(HowNet, WordNet, Wikipedia) ii) \n",
      "Build a conceptual vector or graph model to depict relationship \n",
      "between concept and sentences iii) Apply ranking algorithm to \n",
      "score sentences iv) Generate summaries based on the ranking \n",
      "scores of sentences \n",
      "4.\n",
      "Latent Semantic Analysis Method(LSA) \n",
      "Latent Semantic Analysis(LSA) [13] [14] is a method which \n",
      "excerpt hidden semantic structures of sentences and words \n",
      "that are popularly used in text summarization task.\n",
      "It is an \n",
      "unsupervised learning approach that does not demand any sort \n",
      "of external or training knowledge.\n",
      "LSA captures the text of the \n",
      "input document and excerpt information such as words that \n",
      "frequently occur together and words that are commonly seen in \n",
      "different sentences.\n",
      "A high number of common words amongst \n",
      "the sentences illustrate that the sentences are semantically \n",
      "related.\n",
      "Singular Value Decomposition(SVD) [13], is a method \n",
      "used to find out the interrelations between words and sentences \n",
      "which also has the competence of noise reduction that helps \n",
      "to improve accuracy.\n",
      "SVD, [15] when enforced to document \n",
      "word matrices, can group documents that are semantically \n",
      "associated to one other despite them sharing no common \n",
      "words.\n",
      "The set of words that ensue in connected text is \n",
      "also connected within the same peculiar dimensional space.\n",
      "LSA technique is applied to excerpt the subject-related words \n",
      "and important content conveying sentences from report.\n",
      "The \n",
      "advantage of adopting LSA vectors for summarization over \n",
      "word vectors is that conceptual relations as represented in the \n",
      "human brain are naturally captured in the LSA.\n",
      "Choice of \n",
      "the representative sentence from every scale of the capacity \n",
      "ensures relevancy of sentence to the document and ensures \n",
      "non-redundancy.\n",
      "LS works with text data and the principal \n",
      "ambit due to the collection of topics can be located.\n",
      "Considering an example to depict LSA representatieach \n",
      "otheron, Example 1: Consider following 3 sentences given to \n",
      "LSA based system.\n",
      "dO: 'The man was walked the dog.\n",
      "dl: \n",
      "'The man took the dog to the park in the evening.\n",
      "d2: 'The \n",
      "dog went to the park in the evening.\n",
      "From Fig6 [13] it is to \n",
      "be noted in order that dl is associated to d2 than dO and the \n",
      "conversation 'walked' is linked to the talk 'man' but it is not \n",
      "significant to the word 'park'.\n",
      "These kind of interpretations \n",
      "can be built by using input data and LSA, beyond need for \n",
      "any extraneous awareness.\n",
      "B. SUPERVISED LEARNING METHODS \n",
      "Supervised extractive summarizationrelated techniques are \n",
      "based on a classification approach at sentence level where \n",
      "the system learns by examples to classify between summary \n",
      "and non-summary sentences.\n",
      "The major drawback with the \n",
      "supervised approach is that it requires known manually created \n",
      "summaries by a human to label the sentences in the original \n",
      "training document enclosed with \"summary sentence\" or \"non\u0002summary sentence\" and it also requires more labeled training \n",
      "data for classification.\n",
      "1.\n",
      "Machine Learning Approach based on Bayes rule \n",
      "A set of training documents along with its extractive sum\u0002maries is fed as input to the training stage.\n",
      "The machine \n",
      "learning approach views classification problem in text sum\u0002marization.\n",
      "The sentences are restricted as a non-summary \n",
      "and summary sentence based on the feature possessed by the \n",
      "sentence.\n",
      "The probability of classification are learned from the \n",
      "training data by the following Bayes rule [16]: where s rep\u0002resents the set of sentences in the document and fi represents \n",
      "the features used in classification stage and S represents the \n",
      "set of sentences in the summary.\n",
      "P (s E< SII1,h,h, .... In) \n",
      "represents the probability of the sentences to be included in \n",
      "the summary based on the given features possessed by the \n",
      "sentence.\n",
      "2.\n",
      "Neural Network based approach \n",
      "Fig.\n",
      "8.\n",
      "Neural network after training (a) and after pruning (b) [17] \n",
      "In the approach proposed in [18], RankNet algorautomati\u0002callyithm using neural nets to identify the important sentences \n",
      "in the document.\n",
      "It uses a two-layer neural network with back propagation trained using RankNet algorithm.\n",
      "The first \n",
      "step involves labeling the training data using a machine\u0002learning approach and then extract features of the sentences \n",
      "in both test set and train sets which is then inputted to the \n",
      "neural network system to rank the sentences in the document.\n",
      "Another approach [17]uses a three layered feed-forward neural \n",
      "network which learns in the training stage the characteristics \n",
      "of summary and non-summary sentences.\n",
      "The major phase \n",
      "is the feature fusion phase where the relationship between \n",
      "the features are identified through two stages 1) eliminat\u0002ing infrequent features 2) collapsing frequent features after \n",
      "which sentence ranking is done to identify the important \n",
      "summary sentences.Neural Network [17]after feature fusion \n",
      "is depicted in Fig 8.\n",
      "Dharmendra Hingu, Deep Shah and \n",
      "Sandeep S.Udmale proposed an extractive approach [19]for \n",
      "summarizing the Wikipedia articles by identifying the text \n",
      "features and scoring the sentences by incorporating neural \n",
      "network model [5].\n",
      "This system gets the Wikipedia articles \n",
      "as input followed by tokenisation and stemming.\n",
      "The pre\u0002processed passage is sent to the feature extraction steps, which \n",
      "is based on multiple features of sentences and words.\n",
      "The \n",
      "scores obtained after the feature extraction are fed to the \n",
      "neural network, which produces a single value as output score, \n",
      "signifying the importance of the sentences.\n",
      "Usage of the words \n",
      "and sentences is not considered while assigning the weights \n",
      "which results in less accuracy.\n",
      "3.\n",
      "Conditional Random Fields \n",
      "Conditional Random Fields are a statistical modeling ap\u0002proach that focuses on machine leaning to provide a structured \n",
      "prediction.\n",
      "The proposed system overcomes the issues faced \n",
      "by non-negative matrix Factorization (NMF) methods by in\u0002corporating conditional random fields (CRF) to identify and \n",
      "extract correct features to determine the important sentence \n",
      "of the given text.\n",
      "The main advantage of the method is that \n",
      "it is able to identify correct features and provides a better \n",
      "representation of sentences and groups terms appropriately \n",
      "into its segments.\n",
      "The major drawback of the method is that it \n",
      "focuses on domain-specific which requires an external domain \n",
      "specific corpus for training step, thus this method cannot be \n",
      "applied generically to any document without building a domain \n",
      "corpus which is a time-consuming task.\n",
      "The approach specified \n",
      "in [20]uses CRF as a sequence labelling problem and also \n",
      "captures interaction between sentences through the features \n",
      "extracted for each sentence and it also incorporates complex \n",
      "features such as LSA_scores [21] and lilTS_score [22] but \n",
      "the limitation is that linguistic features are not considered.\n",
      "IV.\n",
      "INFERENCES MADE \n",
      "â€¢ Abounding variations of the extractive path [15] have \n",
      "been focused in the prior ten years.\n",
      "However, it is difficult \n",
      "to say how analytical improvement (sentence or text \n",
      "level) devote to work.\n",
      "â€¢ Beyond NLP, the achieved summary might endure a lack \n",
      "of semantics and cohesion.\n",
      "In texts consist of numerous \n",
      "topics, the provoked summary may not be fair.\n",
      "Conclusive proper weights for respective features is vital to the \n",
      "quality of concluding summary depends on it.\n",
      "â€¢ Feature weights should be given more importance be\u0002cause it plays a major role in choosing key sentences.\n",
      "In text Summarization, the most challenging task is \n",
      "to summarize the contented from a number of semi\u0002structured sources and textual, which includes web pages \n",
      "and databases, in the proper way (size, format, time, \n",
      "language,) for an explicit user.\n",
      "â€¢ Text summary software should crop effective summary \n",
      "within a fewer amount of redundancy and time.\n",
      "Summa\u0002rization appraise using extrinsic or intrinsic part.\n",
      "â€¢ Intrinsic parts pursuit to measure summary nature adopt\u0002ing human evaluation whereas, extrinsic parts measure \n",
      "the same over a effort-based work measure being the \n",
      "information rehabilitation-oriented task.\n",
      "V. EVALUATION METRICS \n",
      "Numerous benchmarking datasets [1] are used for experi\u0002mental evaluation of extractive summarization.\n",
      "Document Un\u0002derstanding Conferences (DUC) is the most common bench\u0002marking datasets used for text summarization.\n",
      "There are a \n",
      "number of datasets like TIPSTER, TREC , TAC , DUC, CNN.\n",
      "It contains documents along with their summaries that are \n",
      "created automatically, manually and submitted summaries[20].\n",
      "From papers surveyed within the previous sections et al in \n",
      "literature, it's been found that agreement between human \n",
      "summarizers is sort of low, each for evaluating and generating \n",
      "summaries quite the shape of the outline, it is tough to judge \n",
      "the outline content.\n",
      "i) Human Evaluation \n",
      "Human judgment usually has wide variance on what's \n",
      "thought-about a \"good\" outline, which implies that creating \n",
      "the analysis method automatic is especially tough.\n",
      "Manual \n",
      "analysis is used, however, this can be each time and labor\u0002intensive because it needs humans to browse not solely the \n",
      "summaries however conjointly the supply documents.\n",
      "Other \n",
      "issues are those regarding coherence and coverage.\n",
      "ii) Rouge \n",
      "Formally, ROUGE-N is an n-gram recall between a candi\u0002date summary and a set of reference summaries.\n",
      "ROUGE-N \n",
      "is computed as follows: \n",
      "ROUGE-N = LSEreference_summaries LN-grams Countmatch{N-gram) \n",
      "L S Ereference_summaries LN-grams Count{N -gram) \n",
      "where, n stands for the length of the n-gram Countmatch(N\u0002gram) is the maximum number of n-grams co-occurring in \n",
      "a candidate summary and a set of reference summaries.\n",
      "Count(N-gram) is the number of N-grams in the set of \n",
      "reference summaries.\n",
      "... ) R 11 R _ ISref n Scandl \n",
      "m eca - ISrefl \n",
      "where Sref n Scand indicates the number of sentences that \n",
      "co-occur in both reference and candidate summaries.\n",
      "tV rectswn = I I Scand \n",
      ") _ 2(Precision)(Recall) \n",
      "v F -measure F - .\n",
      ".\n",
      "all PreCISlOn + Rec \n",
      "vi) Compression Ratio Cr = Slen .\n",
      "Iien \n",
      "where, Slen and Iien are the length of summary and source \n",
      "text respectively.\n",
      "VI.\n",
      "CHALLENGES AND FUTURE RESEARCH \n",
      "DIRECTIONS \n",
      "Evaluating summaries (either automatically or manually) is \n",
      "a difficult task.\n",
      "The main problem in evaluation comes from \n",
      "the impossibility of building a standard against which the \n",
      "results of the systems that have to be compared.\n",
      "Further, it \n",
      "is very hard to find out what a correct summary is because \n",
      "there is a chance of the system to generate a better summary \n",
      "that is different from any human summary which is used as an \n",
      "approximation to correct output.\n",
      "Content choice [23] is not a \n",
      "settled problem.\n",
      "People are completely different and subjective \n",
      "authors would possibly select completely different sentences.\n",
      "Two distinct sentences expressed in disparate words will \n",
      "specific a similar can explicit the same meaning also known \n",
      "as paraphrasing.\n",
      "There exists an approach to automatically \n",
      "evaluate summaries using paraphrases (paraEval).\n",
      "Most text \n",
      "summarization systems perform extractive summarization ap\u0002proach (selecting and photocopying extensive sentences from \n",
      "the professional documents).\n",
      "Though humans can cut and paste \n",
      "relevant data from a text, most of the times they rephrase sen\u0002tences whenever necessary, or they may join different related \n",
      "data into one sentence.\n",
      "The low inter-annotator agreement \n",
      "figures observed during manual evaluations suggest that the \n",
      "future of this research area massively depends on the capacity \n",
      "to find efficient ways of automatically evaluating the systems.\n",
      "VII.\n",
      "CONCLUSION \n",
      "This review has shown assorted mechanism of extractive \n",
      "text summarization process.\n",
      "Extractive summarization process \n",
      "is highly coherent, less redundant and cohesive (summary and \n",
      "information rich).\n",
      "The aim is to give a comprehensive review \n",
      "and comparison of distinctive approaches and techniques of \n",
      "extractive text summarization process.\n",
      "Although research on \n",
      "summarization started way long back, there is still a long way \n",
      "to go.\n",
      "Over the time, focused has drifted from summarizing \n",
      "scientific articles to advertisements, blogs, electronic mail \n",
      "messages and news articles.\n",
      "Simple eradication of sentences \n",
      "has composed satisfactory results in massive applications.\n",
      "Some trends in automatic evaluation of summary system have \n",
      "been focused.\n",
      "However, the work has not focused the different \n",
      "challenges of extractive text summarization process to its full \n",
      "intensity in premises of time and space complication.\n",
      "REFERENCES \n",
      "[1] N. Moratanch and S. Chitrakala, \"A survey on abstractive text sum\u0002marization,\" in Circuit, Power and Computing Technologies (ICCPCT), \n",
      "2016 International Conference on.\n",
      "IEEE, 2016, pp.\n",
      "1-7.\n",
      "[2] F. Kiyomarsi and F. R. Esfahani, \"Optimizing persian text summarization \n",
      "based on fuzzy logic approach,\" in 2011 International Conference on \n",
      "Intelligent Building and Management, 2011.\n",
      "[3] F. Chen, K. Han, and G. Chen, \"An approach to sentence-selection\u0002based text summarization,\" in TENCON'02.\n",
      "Proceedings.\n",
      "2002 IEEE \n",
      "Region 10 Conference on Computers, Communications, Control and \n",
      "Power Engineering, vol.\n",
      "1.\n",
      "IEEE, 2002, pp.\n",
      "489-493.\n",
      "[4] Y. Sankarasubramaniam, K. Ramanathan, and S. Ghosh, \"Text sum\u0002marization using wikipedia,\" Information Processing & Management, \n",
      "vol.\n",
      "50, no.\n",
      "3, pp.\n",
      "443-461, 2014.\n",
      "[5] J. M. Kleinberg, \"Authoritative sources in a hyperlinked environment,\" \n",
      "Journal of the ACM (JACM), vol.\n",
      "46, no.\n",
      "5, pp.\n",
      "604--632, 1999.\n",
      "[6] G. Erkan and D. R. Radev, \"Lexrank: Graph-based lexical centrality \n",
      "as salience in text summarization,\" Journal of Artificial Intelligence \n",
      "Research, pp.\n",
      "457-479, 2004.\n",
      "[7] S. M. R .. W. T. L., Brin, \"The pagerank citation ranking: Bringing \n",
      "order to the web,\" Technical report, Stanford University, Stanford, CA., \n",
      "Tech.\n",
      "Rep., (1998).\n",
      "[8] (2004) Document understanding conferences dataset.\n",
      "Online.\n",
      "[Online].\n",
      "Available: http://duc.nist.gov/data.htrnl.\n",
      "[9] F. Kyoomarsi, H. Khosravi, E. Eslami, P. K. Dehkordy, and A. Tajoddin, \n",
      "\"Optimizing text summarization based on fuzzy logic,\" in Seventh \n",
      "IEEElACIS International Conference on Computer and Information \n",
      "Science.\n",
      "IEEE, 2008, pp.\n",
      "347-352.\n",
      "[10] L. SUanmali, M. S. Binwahlan, and N. Salim, \"Sentence features fusion \n",
      "for text summarization using fuzzy logic,\" in Hybrid Intelligent Systems, \n",
      "2009.\n",
      "HIS'09.\n",
      "Ninth International Conference on, vol.\n",
      "1.\n",
      "IEEE, 2009, \n",
      "pp.\n",
      "142-146.\n",
      "[11] L. Suanmali, N. Salim, and M. S. Binwahlan, \"Fuzzy logic based method \n",
      "for improving text summarization,\" arXiv pre print arXiv:0906.4690, \n",
      "2009.\n",
      "[12] X. W. Meng Wang and C. Xu, \"An approach to concept oriented text \n",
      "summarization,\" in in Proceedings of ISClTS05, IEEE international \n",
      "conference, China,1290-1293\" 2005.\n",
      "[13] M. G. Ozsoy, F. N. Alpaslan, and 1.\n",
      "Cicekli, \"Text summarization using \n",
      "latent semantic analysis,\" Journal of Information Science, vol.\n",
      "37, no.\n",
      "4, \n",
      "pp.\n",
      "405-417, 2011.\n",
      "[14] 1.\n",
      "Mashechkin, M. Petrovskiy, D. Popov, and D. V. Tsarev, \"Automatic \n",
      "text summarization using latent semantic analysis,\" Programming and \n",
      "Computer Software, vol.\n",
      "37, no.\n",
      "6, pp.\n",
      "299-305, 2011.\n",
      "[15] V. Gupta and G. S. Lehal, \"A survey of text summarization extractive \n",
      "techniques,\" Journal of emerging technologies in web intelligence, vol.\n",
      "2, \n",
      "no.\n",
      "3, pp.\n",
      "258-268, 2010.\n",
      "[16] J. L. Neto, A. A. Freitas, and C. A. Kaestner, \"Automatic text summa\u0002rization using a machine learning approach,\" in Advances in Artificial \n",
      "Intelligence.\n",
      "Springer, 2002, pp.\n",
      "205-215.\n",
      "[17] K. Kaikhah, \"Automatic text summarization with neural networks,\" \n",
      "2004.\n",
      "[18] K. M. Svore, L. Vanderwende, and C. J. Burges, \"Enhancing single\u0002document summarization by combining ranknet and third-party sources.\"\n",
      "in EMNLP-CoNLL, 2007, pp.\n",
      "448-457.\n",
      "[19] D. Hingu, D. Shah, and S. S. Udmale, \"Automatic text summarization \n",
      "of wikipedia articles,\" in Communication, Information & Computing \n",
      "Technology (ICCICT), 2015 International Conference on.\n",
      "IEEE,2015, \n",
      "pp.I-4.\n",
      "[20] D. Shen, J.-T.\n",
      "Sun, H. Li, Q. Yang, and Z. Chen, \"Document summa\u0002rization using conditional random fields.\"\n",
      "in IJCAI, vol.\n",
      "7, 2007, pp.\n",
      "2862-2867.\n",
      "[21] Y. Gong and X. Liu, \"Generic text summarization using relevance \n",
      "measure and latent semantic analysis,\" in Proceedings of the 24th annual \n",
      "international ACM SIGIR conference on Research and development in \n",
      "information retrieval.\n",
      "ACM, 2001, pp.\n",
      "19-25.\n",
      "[22] R. Mihalcea, \"Language independent extractive summarization,\" in \n",
      "Proceedings of the ACL 2005 on Interactive poster and demonstration \n",
      "sessions.\n",
      "Association for Computational Linguistics, 2005, pp.\n",
      "49-52.\n",
      "[23] N. Lalithamani, R. Sukumaran, K. Alagamrnai, K. K. Sowmya, V. Di\u0002vyalakshmi, and S. Shanmugapriya, ''A mixed-initiative approach for \n",
      "summarizing discussions coupled with sentimental analysis,\" in Proceed\u0002ings of the 2014 International Conference on Interdisciplinary Advances \n",
      "in Applied Computing.\n",
      "ACM, 2014, p.\n",
      "5.\n"
     ]
    }
   ],
   "source": [
    "for i in sentences_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate term-document matrix (TD matrix) of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of bow matrix <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of the matrix <bound method spmatrix.get_shape of <249x1214 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3439 stored elements in Compressed Sparse Row format>>\n",
      "Size of the matrix is: 48\n",
      "['02', '09', '0906', '10', '11', '12', '1290', '1293', '13', '14', '142', '146', '15', '16', '17', '18', '19', '1998', '1999', '20', '2001', '2002', '2004', '2005', '2007', '2008', '2009', '2010', '2011', '2014', '2015', '2016', '205', '21', '215', '22', '23', '24th', '25', '258', '268', '2862', '2867', '299', '305', '347', '352', '37', '405', '417', '443', '448', '457', '46', '461', '4690', '479', '489', '49', '493', '50', '52', '604', '632', 'able', 'abounding', 'about', 'abstract', 'abstractive', 'accomplishes', 'accuracy', 'achieved', 'acl', 'acm', 'adhesive', 'adjectives', 'adopt', 'adopting', 'advance', 'advances', 'advantage', 'adverbs', 'advertisements', 'after', 'against', 'age', 'agreement', 'aim', 'al', 'alagamrnai', 'algorautomati', 'algorithm', 'all', 'along', 'alpaslan', 'already', 'also', 'alternative', 'although', 'ambit', 'amongst', 'amount', 'an', 'analysis', 'analytical', 'anaphora', 'and', 'annotator', 'annual', 'another', 'any', 'ap', 'applications', 'applied', 'apply', 'appraise', 'approach', 'approaches', 'appropriate', 'appropriately', 'approximation', 'are', 'area', 'arizona', 'articles', 'artificial', 'arxiv', 'as', 'ascertained', 'aspect', 'assigned', 'assigning', 'associated', 'association', 'assorted', 'at', 'attention', 'authentic', 'authoritative', 'authors', 'automatic', 'automatically', 'automation', 'available', 'awareness', 'awe', 'back', 'base', 'based', 'basic', 'bayes', 'be', 'because', 'been', 'beginning', 'being', 'bench', 'benchmarking', 'better', 'between', 'beyond', 'biased', 'big', 'binwahlan', 'bipartite', 'blogs', 'books', 'both', 'brain', 'brin', 'bringing', 'broadly', 'browse', 'build', 'building', 'builds', 'built', 'burges', 'but', 'by', 'ca', 'calculated', 'callyithm', 'can', 'canada', 'candi', 'candidate', 'cannot', 'capable', 'capacity', 'captured', 'captures', 'case', 'cause', 'centrality', 'centroid', 'certain', 'challenges', 'challenging', 'chance', 'chances', 'characteristics', 'chen', 'china', 'chitrakala', 'choice', 'choosing', 'cicekli', 'circuit', 'citation', 'classification', 'classified', 'classify', 'closer', 'clustered', 'cnn', 'co', 'coarse', 'coherence', 'coherency', 'coherent', 'cohesion', 'cohesive', 'collapsing', 'collection', 'combines', 'combining', 'comes', 'common', 'commonly', 'communication', 'communications', 'compared', 'comparison', 'compensation', 'competence', 'completely', 'complex', 'complication', 'components', 'composed', 'comprehensive', 'compression', 'computational', 'computed', 'computer', 'computers', 'computing', 'concatenating', 'concept', 'concepts', 'conceptual', 'concise', 'concluding', 'conclusion', 'conclusive', 'concordia', 'condense', 'conditional', 'conference', 'conferences', 'conferred', 'conjointly', 'conll', 'connected', 'consider', 'considered', 'considering', 'consist', 'consists', 'consuming', 'contains', 'content', 'contented', 'context', 'contribute', 'control', 'conversation', 'convey', 'conveying', 'corporating', 'corpus', 'correct', 'cosine', 'count', 'countenance', 'countmatch', 'coupled', 'coverage', 'cr', 'created', 'creating', 'crf', 'crop', 'crucial', 'cue', 'cuses', 'cut', 'czech', 'd2', 'dangling', 'data', 'databases', 'dataset', 'datasets', 'date', 'deciding', 'decomposition', 'deep', 'defuzzifier', 'degree', 'dehkordy', 'demand', 'demonstration', 'denotation', 'denouement', 'depends', 'depict', 'depicted', 'depicts', 'depth', 'derstanding', 'describe', 'describes', 'desire', 'desired', 'despite', 'detail', 'detected', 'determine', 'determined', 'develop', 'development', 'devote', 'dharmendra', 'di', 'different', 'difficult', 'digest', 'dimensional', 'directions', 'discussed', 'discussions', 'disparate', 'distinct', 'distinctive', 'dl', 'do', 'document', 'documents', 'does', 'dog', 'domain', 'done', 'drawback', 'drifted', 'duc', 'due', 'during', 'each', 'earlier', 'eca', 'edge', 'edges', 'effective', 'efficient', 'efficiently', 'effort', 'eigen', 'either', 'electronic', 'eliminat', 'els', 'emergent', 'emerging', 'emnlp', 'employed', 'en', 'enclosed', 'end', 'endure', 'enforced', 'engine', 'engineering', 'enhancements', 'enhancing', 'ensue', 'ensures', 'environment', 'eradication', 'ereference_summaries', 'erkan', 'esfahani', 'eslami', 'especially', 'essential', 'et', 'etc', 'evaluate', 'evaluated', 'evaluating', 'evaluation', 'evaluations', 'even', 'evening', 'event', 'every', 'example', 'examples', 'except', 'excerpt', 'exclusive', 'exists', 'experi', 'explicit', 'exploration', 'expressed', 'extensive', 'extensively', 'external', 'extrac', 'extract', 'extracted', 'extraction', 'extractive', 'extraneous', 'extrinsic', 'faced', 'factorization', 'faculty', 'fair', 'far', 'fascinating', 'feature', 'features', 'fed', 'feed', 'fewer', 'fi', 'fields', 'fig', 'fig6', 'figures', 'final', 'finally', 'find', 'finest', 'first', 'fitted', 'flow', 'fo', 'focused', 'focuses', 'followed', 'following', 'follows', 'for', 'form', 'formally', 'format', 'forward', 'found', 'four', 'framework', 'freitas', 'frequent', 'frequently', 'from', 'full', 'further', 'fusion', 'future', 'fuzzifier', 'fuzzy', 'gains', 'generally', 'generate', 'generated', 'generating', 'generic', 'generically', 'germany', 'gets', 'ghosh', 'gine', 'give', 'given', 'global', 'go', 'gong', 'good', 'gov', 'gram', 'grams', 'graph', 'graphs', 'greater', 'group', 'groups', 'grown', 'growth', 'gupta', 'han', 'handled', 'hard', 'has', 'have', 'helps', 'hence', 'hidden', 'hierarchically', 'high', 'higher', 'highlights', 'highly', 'hingu', 'his', 'hits', 'how', 'however', 'hownet', 'htrnl', 'http', 'human', 'humans', 'hybrid', 'hyperlinked', 'iccict', 'iccpct', 'identified', 'identify', 'identifying', 'idf', 'ieee', 'ieeelacis', 'ihr', 'ii', 'iien', 'iii', 'ijcai', 'illustrate', 'implication', 'implies', 'importance', 'important', 'impossibility', 'improve', 'improvement', 'improving', 'in', 'included', 'includes', 'incorporates', 'incorporating', 'independent', 'index', 'indicate', 'indicates', 'indicators', 'inference', 'inferences', 'infor', 'informa', 'informatics', 'information', 'infrequent', 'ing', 'ings', 'initial', 'initiative', 'input', 'inputted', 'inspiring', 'instead', 'intelligence', 'intelligent', 'intensity', 'intensive', 'inter', 'interaction', 'interactive', 'interdisciplinary', 'international', 'internet', 'interpretations', 'interprets', 'interrelations', 'into', 'intrinsic', 'introduction', 'inundation', 'involve', 'involves', 'is', 'isclts05', 'isref', 'isrefl', 'issues', 'it', 'iterative', 'its', 'iv', 'jacm', 'join', 'journal', 'judge', 'judgment', 'just', 'kaestner', 'kaikhah', 'keeping', 'key', 'keyword', 'keywords', 'khosravi', 'kind', 'kiyomarsi', 'kleinberg', 'knowl', 'knowledge', 'known', 'kyoomarsi', 'l2', 'lab', 'label', 'labeled', 'labeling', 'labelling', 'labor', 'lack', 'ladda', 'lalithamani', 'language', 'latent', 'later', 'latest', 'launch', 'layer', 'layered', 'lead', 'leaning', 'learned', 'learning', 'learns', 'lehal', 'leipzig', 'length', 'less', 'level', 'lexical', 'lexrank', 'li', 'like', 'likely', 'lilts_score', 'limitation', 'linguistic', 'linguistics', 'linked', 'list', 'literature', 'liu', 'ln', 'located', 'location', 'logic', 'long', 'longest', 'low', 'lower', 'ls', 'lsa', 'lsa_scores', 'lsereference_summaries', 'machine', 'made', 'magazine', 'mail', 'main', 'mainly', 'maintain', 'major', 'man', 'management', 'manual', 'manually', 'manuscript', 'many', 'maps', 'maries', 'marization', 'market', 'marking', 'marvelous', 'masaryk', 'mashechkin', 'massive', 'massively', 'mation', 'matrices', 'matrix', 'maximum', 'may', 'meaning', 'measure', 'measures', 'mechanism', 'meeting', 'meng', 'mental', 'mentioned', 'messages', 'meth', 'method', 'methodologies', 'methodology', 'methods', 'metrics', 'might', 'mihalcea', 'minimum', 'mixed', 'mod', 'model', 'modeling', 'models', 'montreal', 'moratanch', 'more', 'most', 'mostly', 'multiple', 'naturally', 'nature', 'necessary', 'need', 'needs', 'negative', 'neto', 'nets', 'network', 'networks', 'neural', 'news', 'newspaper', 'nexus', 'ninth', 'nist', 'nlp', 'nmf', 'no', 'noise', 'non', 'normalized', 'not', 'noted', 'noun', 'nouns', 'number', 'numerical', 'numerous', 'objective', 'observed', 'obtain', 'obtained', 'obtaining', 'occur', 'occurring', 'ods', 'of', 'offered', 'on', 'one', 'online', 'optimizing', 'or', 'order', 'ordering', 'organized', 'oriented', 'origin', 'original', 'other', 'otheron', 'out', 'outline', 'outperforms', 'output', 'over', 'overcomes', 'oversized', 'overview', 'overwhelms', 'ozsoy', 'pa', 'pagerank', 'pages', 'paper', 'papers', 'paraeval', 'paragraph', 'paragraphs', 'paraphrases', 'paraphrasing', 'park', 'part', 'parts', 'party', 'passage', 'paste', 'path', 'peculiar', 'people', 'perform', 'performance', 'peripheral', 'pers', 'persian', 'persistently', 'petrovskiy', 'phase', 'photocopying', 'phrase', 'phrases', 'physically', 'piece', 'plays', 'popov', 'popularly', 'populous', 'position', 'possessed', 'possibly', 'poster', 'power', 'pp', 'pre', 'precise', 'precision', 'precislon', 'predefined', 'prediction', 'premises', 'preserves', 'previous', 'principal', 'print', 'prior', 'proach', 'probability', 'problem', 'proceed', 'proceedings', 'process', 'processed', 'processing', 'produce', 'produces', 'professional', 'programming', 'progressive', 'propagation', 'proper', 'proposed', 'proved', 'provide', 'provides', 'provoked', 'pruning', 'publicly', 'pursuit', 'quality', 'quite', 'radev', 'ramanatha', 'ramanathan', 'random', 'rank', 'ranked', 'ranking', 'ranknet', 'rapid', 'ratio', 'reading', 'rec', 'recall', 'recent', 'rectswn', 'reduce', 'reduced', 'reduction', 'redundancy', 'redundant', 'reference', 'references', 'regarding', 'region', 'rehabilitation', 'related', 'relations', 'relationship', 'relatively', 'relevance', 'relevancy', 'relevant', 'reliable', 'rely', 'rep', 'repetition', 'rephrase', 'report', 'reports', 'represent', 'representatieach', 'representation', 'representative', 'represented', 'represents', 'reproduce', 'republic', 'requirement', 'requires', 'research', 'resents', 'respective', 'respectively', 'restricted', 'results', 'retrieval', 'retrieve', 'retrieved', 'review', 'reviewed', 'revision', 'rich', 'rization', 'role', 'rouge', 'rough', 'rule', 'salience', 'salient', 'salim', 'same', 'sandeep', 'sankarasubramaniam', 'satisfactory', 'say', 'scale', 'scand', 'scandl', 'scheme', 'science', 'scientific', 'score', 'scores', 'scoring', 'section', 'sections', 'seeking', 'seen', 'segments', 'select', 'selecting', 'selection', 'semantic', 'semantically', 'semantics', 'semi', 'sen', 'sent', 'sentence', 'sentenced', 'sentences', 'sentimental', 'sequence', 'serve', 'sessions', 'set', 'sets', 'settled', 'seventh', 'shah', 'shanmugapriya', 'shape', 'sharing', 'shen', 'shorter', 'should', 'shown', 'shows', 'sigir', 'significance', 'significant', 'signifying', 'sii1', 'similar', 'similarity', 'simple', 'since', 'single', 'singular', 'size', 'sketch', 'slen', 'software', 'solely', 'some', 'sophisticated', 'sort', 'source', 'sources', 'sowmya', 'space', 'specific', 'specified', 'speedy', 'sport', 'springer', 'sref', 'stage', 'stages', 'standard', 'stands', 'stanford', 'started', 'state', 'statistical', 'stemming', 'step', 'steps', 'still', 'stock', 'stories', 'strongly', 'struc', 'structure', 'structured', 'structures', 'suanmali', 'subject', 'subjective', 'submitted', 'successful', 'such', 'suggest', 'suitable', 'sukumaran', 'sum', 'summa', 'summaries', 'summariza', 'summarization', 'summarizationrelated', 'summarize', 'summarized', 'summarizer', 'summarizers', 'summarizes', 'summarizing', 'summary', 'summed', 'sun', 'supervised', 'supply', 'supporting', 'survey', 'surveyed', 'svd', 'svore', 'system', 'systems', 'tac', 'tajoddin', 'talk', 'task', 'tech', 'technical', 'technique', 'techniques', 'technologies', 'technology', 'ten', 'tence', 'tences', 'tencon', 'term', 'termed', 'terms', 'test', 'text', 'texts', 'textual', 'tf', 'than', 'that', 'the', 'their', 'them', 'theme', 'then', 'there', 'therefore', 'these', 'they', 'third', 'this', 'those', 'though', 'thought', 'three', 'through', 'thus', 'time', 'times', 'tion', 'tipster', 'title', 'to', 'together', 'tokenisation', 'took', 'topic', 'topics', 'tough', 'towards', 'train', 'trained', 'training', 'trec', 'tremendous', 'trends', 'tsarev', 'ture', 'tv', 'twin', 'two', 'types', 'udmale', 'un', 'understand', 'understanding', 'undirected', 'unicef', 'uniqueness', 'universities', 'university', 'unsupervised', 'up', 'upper', 'uppercase', 'usa', 'usage', 'used', 'user', 'uses', 'using', 'usually', 'utilizes', 'value', 'values', 'vanderwende', 'variance', 'variation', 'variations', 'various', 'vector', 'vectors', 'verbs', 'version', 'versions', 'very', 'vi', 'views', 'vii', 'vital', 'vol', 'volume', 'vyalakshmi', 'walked', 'wang', 'was', 'way', 'ways', 'wealth', 'web', 'weight', 'weighted', 'weights', 'went', 'what', 'when', 'whenever', 'where', 'whereas', 'which', 'while', 'wide', 'wikipedia', 'will', 'with', 'within', 'without', 'word', 'wordnet', 'words', 'work', 'working', 'works', 'would', 'xu', 'yang', 'years', 'yields']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('The data type of bow matrix {}'.format(type(cv_matrix)))\n",
    "print('Shape of the matrix {}'.format(cv_matrix.get_shape))\n",
    "print('Size of the matrix is: {}'.format(sys.getsizeof(cv_matrix)))\n",
    "print(cv.get_feature_names())\n",
    "print(cv_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n",
    "print(normal_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _cs_matrix.toarray of <1214x249 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 3439 stored elements in Compressed Sparse Column format>>\n"
     ]
    }
   ],
   "source": [
    "print(normal_matrix.T.toarray)\n",
    "res_graph = normal_matrix * normal_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges 14582\n",
      "Number of vertices 249\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory used by the graph in Bytes is: 48\n"
     ]
    }
   ],
   "source": [
    "nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n",
    "nx.draw_circular(nx_graph)\n",
    "print('Number of edges {}'.format(nx_graph.number_of_edges()))\n",
    "print('Number of vertices {}'.format(nx_graph.number_of_nodes()))\n",
    "plt.show()\n",
    "print('The memory used by the graph in Bytes is: {}'.format(sys.getsizeof(nx_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Getting the rank of every sentence using textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "The size used by the dictionary in Bytes is: 9312\n",
      "0 0.005152606645907651\n",
      "1 0.005288835343110857\n",
      "2 0.006407700687137224\n",
      "3 0.00469333418315987\n",
      "4 0.004385547417880532\n",
      "5 0.005566537955541965\n",
      "6 0.004365163828970424\n",
      "7 0.004064091378643071\n",
      "8 0.0038459988517411487\n",
      "9 0.0053909069002482215\n",
      "10 0.0035905078642940054\n",
      "11 0.0041665652321846635\n",
      "12 0.0022503715974677386\n",
      "13 0.004505889805955317\n",
      "14 0.0032153440625236613\n",
      "15 0.004024489385087385\n",
      "16 0.002838916069346994\n",
      "17 0.0033059840783119326\n",
      "18 0.0051485039919525084\n",
      "19 0.0056320639439613705\n",
      "20 0.004473717295651369\n",
      "21 0.004468558982911423\n",
      "22 0.004188450186808081\n",
      "23 0.006089778188029934\n",
      "24 0.0036863336869304352\n",
      "25 0.0025806391641432372\n",
      "26 0.004302363276815619\n",
      "27 0.0019966692842818892\n",
      "28 0.005649373057655497\n",
      "29 0.004381730718598641\n",
      "30 0.0055485378341249385\n",
      "31 0.005233508640862229\n",
      "32 0.005820365042434312\n",
      "33 0.0043650752723679714\n",
      "34 0.007260569678591901\n",
      "35 0.0054399382323156616\n",
      "36 0.003420193812597015\n",
      "37 0.002207637129536699\n",
      "38 0.005142054570501476\n",
      "39 0.0043109697900337645\n",
      "40 0.005064680121360816\n",
      "41 0.005305869658531604\n",
      "42 0.0070605018491472415\n",
      "43 0.0006303845345660854\n",
      "44 0.007584770359323023\n",
      "45 0.004131983376458643\n",
      "46 0.0038942738635663796\n",
      "47 0.008697423678251248\n",
      "48 0.0026557312854008385\n",
      "49 0.00564131200212925\n",
      "50 0.005941945028233742\n",
      "51 0.002578409418322143\n",
      "52 0.004601603411393033\n",
      "53 0.003194749207231683\n",
      "54 0.00386124870762577\n",
      "55 0.006410421405178167\n",
      "56 0.003833103269847765\n",
      "57 0.0032070521393063405\n",
      "58 0.0018537969861960794\n",
      "59 0.0006303845345660854\n",
      "60 0.002922305659361888\n",
      "61 0.0040743697871486505\n",
      "62 0.003337807848940604\n",
      "63 0.0048756465685371704\n",
      "64 0.005607684521143884\n",
      "65 0.005807925872404992\n",
      "66 0.006225943376334439\n",
      "67 0.0056842771216743335\n",
      "68 0.003479600959736649\n",
      "69 0.004650810375232379\n",
      "70 0.0006303845345660854\n",
      "71 0.0029304542077736996\n",
      "72 0.005150619941698228\n",
      "73 0.005778913145332305\n",
      "74 0.0062402236729433595\n",
      "75 0.004662017509701024\n",
      "76 0.0006303845345660854\n",
      "77 0.004984821504099987\n",
      "78 0.006296105250354547\n",
      "79 0.005755766673857971\n",
      "80 0.0021793764611534733\n",
      "81 0.003105578268404814\n",
      "82 0.004920314006864349\n",
      "83 0.006746971627953528\n",
      "84 0.004806850528812769\n",
      "85 0.0038806663831291825\n",
      "86 0.006309115945100787\n",
      "87 0.0050449132404026585\n",
      "88 0.004917176231705347\n",
      "89 0.0036643222179080217\n",
      "90 0.004426161222082308\n",
      "91 0.00453795939017206\n",
      "92 0.00517376191294993\n",
      "93 0.005510859046841464\n",
      "94 0.004659585863031697\n",
      "95 0.002718183651745385\n",
      "96 0.0032177830464443463\n",
      "97 0.005344685139836793\n",
      "98 0.004913772427302305\n",
      "99 0.005908707648880114\n",
      "100 0.003226010543782756\n",
      "101 0.005565454028669082\n",
      "102 0.006676749076981289\n",
      "103 0.0006303845345660854\n",
      "104 0.0048871857697928645\n",
      "105 0.004075242490707232\n",
      "106 0.007427210126524895\n",
      "107 0.008068359301315331\n",
      "108 0.008207480234188491\n",
      "109 0.0006303845345660854\n",
      "110 0.003225935839301615\n",
      "111 0.0006303845345660854\n",
      "112 0.0056376727281488405\n",
      "113 0.0022915194783638786\n",
      "114 0.007389083784268242\n",
      "115 0.005034455613928345\n",
      "116 0.005786928453066765\n",
      "117 0.005620933678820631\n",
      "118 0.0034904724347487254\n",
      "119 0.006208663724398748\n",
      "120 0.005967805399999541\n",
      "121 0.005302625407364257\n",
      "122 0.0006303845345660854\n",
      "123 0.0024963185954284184\n",
      "124 0.0057458879947476895\n",
      "125 0.005656276458242602\n",
      "126 0.004424200497391391\n",
      "127 0.005865686536359848\n",
      "128 0.002578409418322143\n",
      "129 0.003651055555391046\n",
      "130 0.003511673240341107\n",
      "131 0.0031823344365832277\n",
      "132 0.0037174468121215186\n",
      "133 0.004469100545224732\n",
      "134 0.003021870085791838\n",
      "135 0.005899198491577512\n",
      "136 0.0032842982530213276\n",
      "137 0.0019074983735777814\n",
      "138 0.0032865489268690574\n",
      "139 0.0032539930230924903\n",
      "140 0.004023460058524676\n",
      "141 0.0024798355900402897\n",
      "142 0.003456507433803868\n",
      "143 0.005547192437169716\n",
      "144 0.0031223733326331974\n",
      "145 0.004255612695845769\n",
      "146 0.0022900721166287477\n",
      "147 0.003635346639430599\n",
      "148 0.004218320179036459\n",
      "149 0.005659807001681262\n",
      "150 0.0038348307637977946\n",
      "151 0.0018723832513825908\n",
      "152 0.0006303845345660854\n",
      "153 0.0032726898484487024\n",
      "154 0.004452705200048527\n",
      "155 0.0034458545105291337\n",
      "156 0.0027997039372655687\n",
      "157 0.0059289455206530724\n",
      "158 0.005378916981041743\n",
      "159 0.002247995048612344\n",
      "160 0.0021498681912326084\n",
      "161 0.003067835252702383\n",
      "162 0.002694746667418631\n",
      "163 0.004387823675395436\n",
      "164 0.003511502544987977\n",
      "165 0.004471828688476446\n",
      "166 0.004202563563773901\n",
      "167 0.0035120136050161162\n",
      "168 0.003986247123115418\n",
      "169 0.005604557552832291\n",
      "170 0.0025683386788125325\n",
      "171 0.0028777770801458463\n",
      "172 0.00262337044200083\n",
      "173 0.0033611968242861505\n",
      "174 0.005763508467938918\n",
      "175 0.003579239458500231\n",
      "176 0.0037533362126485805\n",
      "177 0.0006303845345660854\n",
      "178 0.0050075617627854\n",
      "179 0.004252326701886053\n",
      "180 0.0017918232813572163\n",
      "181 0.0028089656655541874\n",
      "182 0.0006303845345660854\n",
      "183 0.004023731309290689\n",
      "184 0.004202563563773901\n",
      "185 0.003023896036385966\n",
      "186 0.003372139892683097\n",
      "187 0.004955528896979462\n",
      "188 0.002419098560482366\n",
      "189 0.0033934714282267357\n",
      "190 0.003372139892683097\n",
      "191 0.004955528896979462\n",
      "192 0.004202563563773901\n",
      "193 0.004829229265532286\n",
      "194 0.002987515788631786\n",
      "195 0.0026428959717728173\n",
      "196 0.0028279044026250887\n",
      "197 0.0022011223765767834\n",
      "198 0.004202563563773902\n",
      "199 0.004202563563773902\n",
      "200 0.0017308419499321194\n",
      "201 0.004192770326069977\n",
      "202 0.0036102575011797973\n",
      "203 0.004202563563773901\n",
      "204 0.004179282032138963\n",
      "205 0.004202563563773901\n",
      "206 0.002505342210813537\n",
      "207 0.0006303845345660854\n",
      "208 0.0038698942755803225\n",
      "209 0.004202563563773901\n",
      "210 0.0029999223994937635\n",
      "211 0.004695253254748944\n",
      "212 0.001989520127490488\n",
      "213 0.003815440384835244\n",
      "214 0.003962370175701847\n",
      "215 0.004955528896979461\n",
      "216 0.002552256452727402\n",
      "217 0.0024670665463612843\n",
      "218 0.0034661869455458226\n",
      "219 0.003962370175701847\n",
      "220 0.004955528896979461\n",
      "221 0.0025522564527274026\n",
      "222 0.00433802646617263\n",
      "223 0.004083709548016925\n",
      "224 0.004955528896979461\n",
      "225 0.004202563563773901\n",
      "226 0.0037484895600839187\n",
      "227 0.0031391356356395327\n",
      "228 0.004202563563773901\n",
      "229 0.0031715441629783527\n",
      "230 0.002686703003008648\n",
      "231 0.0033854993496255075\n",
      "232 0.0030205440486927274\n",
      "233 0.004390367544361194\n",
      "234 0.0037583697314594673\n",
      "235 0.002186152535499987\n",
      "236 0.0025460420643405608\n",
      "237 0.0024863694105432452\n",
      "238 0.003564743209053844\n",
      "239 0.004202563563773901\n",
      "240 0.0055069307954245875\n",
      "241 0.003414915116555079\n",
      "242 0.0021361665685702617\n",
      "243 0.004088342443748225\n",
      "244 0.0027962747570167827\n",
      "245 0.004202563563773901\n",
      "246 0.0039010713170030305\n",
      "247 0.0024882775214803655\n",
      "248 0.0006303845345660854\n"
     ]
    }
   ],
   "source": [
    "ranks = nx.pagerank(nx_graph)\n",
    "\n",
    "print(type(ranks))\n",
    "print('The size used by the dictionary in Bytes is: {}'.format(sys.getsizeof(ranks)))\n",
    "\n",
    "for i in ranks:\n",
    "    print(i, ranks[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Finding important sentences and generating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n",
    "sentence_array = np.asarray(sentence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_max = float(sentence_array[0][0])\n",
    "rank_min = float(sentence_array[len(sentence_array) - 1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008697423678251248\n",
      "0.0006303845345660854\n"
     ]
    }
   ],
   "source": [
    "print(rank_max)\n",
    "print(rank_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "temp_array = []\n",
    "\n",
    "flag = 0\n",
    "if rank_max - rank_min == 0:\n",
    "    temp_array.append(0)\n",
    "    flag = 1\n",
    "\n",
    "if flag != 1:\n",
    "    for i in range(0, len(sentence_array)):\n",
    "        temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n",
    "\n",
    "print(len(temp_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = (sum(temp_array) / len(temp_array)) + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "if len(temp_array) > 1:\n",
    "    for i in range(0, len(temp_array)):\n",
    "        if temp_array[i] > threshold:\n",
    "                sentence_list.append(sentence_array[i][1])\n",
    "else:\n",
    "    sentence_list.append(sentence_array[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Writing the summary to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized length of the sentence is calculated \n",
      "as the ratio between a number of words in the sentence to the \n",
      "number of words in the longest sentence in the document. P (s E< SII1,h,h, .... In) \n",
      "represents the probability of the sentences to be included in \n",
      "the summary based on the given features possessed by the \n",
      "sentence. The probability of classification are learned from the \n",
      "training data by the following Bayes rule [16]: where s rep\u0002resents the set of sentences in the document and fi represents \n",
      "the features used in classification stage and S represents the \n",
      "set of sentences in the summary. SENTENCE LEVEL FEATURES \n",
      "2.1 Sentence location feature \n",
      "The sentences that occur in the beginning and the conclusion \n",
      "part of the document are most likely important since most \n",
      "documents are hierarchically structured with important infor\u0002mation in the beginning and the end of the paragraphs. The sentences are restricted as a non-summary \n",
      "and summary sentence based on the feature possessed by the \n",
      "sentence. The first \n",
      "step involves labeling the training data using a machine\u0002learning approach and then extract features of the sentences \n",
      "in both test set and train sets which is then inputted to the \n",
      "neural network system to rank the sentences in the document. 1.2 Title Word feature \n",
      "The sentences in the original document which consists of \n",
      "words mentioned in the title have greater chances to contribute \n",
      "to the final summary since they serve as indicators of the theme \n",
      "of the document. 1.5 Upper case word feature \n",
      "The words which are in uppercase such as \"UNICEF\" are \n",
      "considered to be important words and those sentences that \n",
      "consist of these words are termed important in the context of \n",
      "sentence selection for the final summary. The basic steps in concept \n",
      "based summarization are: i) Retrieve concepts of a text from external knowledge base(HowNet, WordNet, Wikipedia) ii) \n",
      "Build a conceptual vector or graph model to depict relationship \n",
      "between concept and sentences iii) Apply ranking algorithm to \n",
      "score sentences iv) Generate summaries based on the ranking \n",
      "scores of sentences \n",
      "4. The major drawback with the \n",
      "supervised approach is that it requires known manually created \n",
      "summaries by a human to label the sentences in the original \n",
      "training document enclosed with \"summary sentence\" or \"non\u0002summary sentence\" and it also requires more labeled training \n",
      "data for classification. The unsupervised approaches do not \n",
      "need human summaries (user input) in deciding the important \n",
      "features of the document, it requires the most sophisticated \n",
      "algorithm to provide compensation for the lack of human \n",
      "knowledge. It is very \n",
      "crucial for humans to understand and to describe the content \n",
      "of the text. LSA captures the text of the \n",
      "input document and excerpt information such as words that \n",
      "frequently occur together and words that are commonly seen in \n",
      "different sentences. In the methodology proposed [12], the \n",
      "importance of sentences is calculated based on the concepts \n",
      "retrieved from HowNet instead of words. The summary is generated by ordering the \n",
      "ranked sentences in the order they occur in the original \n",
      "document to maintain coherency. The sentences in the \n",
      "document are represented as a graph and the edges between \n",
      "the sentences represents weighted cosine similarity values. The pre\u0002processed passage is sent to the feature extraction steps, which \n",
      "is based on multiple features of sentences and words. The significance \n",
      "of sentences is strongly based on statistical and linguistic \n",
      "features of sentences. The \n",
      "scores obtained after the feature extraction are fed to the \n",
      "neural network, which produces a single value as output score, \n",
      "signifying the importance of the sentences. 2.4 Sentence-to-Sentence Cohesion \n",
      "The cohesion between sentences for every sentence(s), the \n",
      "similarity between s and alternative sentences are calculated \n",
      "which are summed up and coarse value of the aspect is \n",
      "obtained for s. The feature values are normalized between \n",
      "[0, 1] where value closer to 1.0 indicates a higher degree of \n",
      "cohesion between sentences. The main problem in evaluation comes from \n",
      "the impossibility of building a standard against which the \n",
      "results of the systems that have to be compared. From Fig6 [13] it is to \n",
      "be noted in order that dl is associated to d2 than dO and the \n",
      "conversation 'walked' is linked to the talk 'man' but it is not \n",
      "significant to the word 'park'. In text Summarization, the most challenging task is \n",
      "to summarize the contented from a number of semi\u0002structured sources and textual, which includes web pages \n",
      "and databases, in the proper way (size, format, time, \n",
      "language,) for an explicit user. The approach specified \n",
      "in [20]uses CRF as a sequence labelling problem and also \n",
      "captures interaction between sentences through the features \n",
      "extracted for each sentence and it also incorporates complex \n",
      "features such as LSA_scores [21] and lilTS_score [22] but \n",
      "the limitation is that linguistic features are not considered. The sentence that consists of main keywords is most \n",
      "likely included in the final summary. Another graph based approach \n",
      "LexRank [6], where the salience of the sentence is determined \n",
      "by the concept of Eigen vector centrality. The major phase \n",
      "is the feature fusion phase where the relationship between \n",
      "the features are identified through two stages 1) eliminat\u0002ing infrequent features 2) collapsing frequent features after \n",
      "which sentence ranking is done to identify the important \n",
      "summary sentences.Neural Network [17]after feature fusion \n",
      "is depicted in Fig 8. Ladda Suanmali et al [11] proposed fuzzy logic approach \n",
      "is used for automatic text summarization which is the initial \n",
      "step , the text document is pre-processed followed by feature \n",
      "extraction(Title features, Sentence length, Sentence position, \n",
      "Sentence-sentence similarity, term weight, Proper noun and \n",
      "Numerical data. However, the work has not focused the different \n",
      "challenges of extractive text summarization process to its full \n",
      "intensity in premises of time and space complication. A conceptual vector \n",
      "model is built to obtain a rough summarization and similarity \n",
      "measures are calculated between the sentences to reduce \n",
      "redundancy in the final summary. The proposed system overcomes the issues faced \n",
      "by non-negative matrix Factorization (NMF) methods by in\u0002corporating conditional random fields (CRF) to identify and \n",
      "extract correct features to determine the important sentence \n",
      "of the given text. The \n",
      "sentences are clustered into groups based on their similarity \n",
      "measures and then the sentences are ranked based on their \n",
      "LexRank scores similar to PageRank algorithm [7]except that \n",
      "the similarity graph is undirected in LexRank method. Count(N-gram) is the number of N-grams in the set of \n",
      "reference summaries. The main advantage of the method is that \n",
      "it is able to identify correct features and provides a better \n",
      "representation of sentences and groups terms appropriately \n",
      "into its segments. FEATURES FOR EXTRACTIVE TEXT \n",
      "SUMMARIZATION \n",
      "Earlier techniques involve assigning a score to sentences \n",
      "based on a countenance that are predefined based on the \n",
      "methodology applied. A Higher score is \n",
      "assigned to the paragraph in the peripheral sections (beginning \n",
      "and end paragraphs of the document). Neural network after training (a) and after pruning (b) [17] \n",
      "In the approach proposed in [18], RankNet algorautomati\u0002callyithm using neural nets to identify the important sentences \n",
      "in the document. The main advantage \n",
      "of a text summarization is reading time of the user can be \n",
      "reduced.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = \" \".join(str(x) for x in sentence_list)\n",
    "print(summary)\n",
    "f = open('sum.txt', 'a+')\n",
    "f.write('\\n')\n",
    "f.write(summary)\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized length of the sentence is calculated \n",
      "as the ratio between a number of words in the sentence to the \n",
      "number of words in the longest sentence in the document.\n",
      "P (s E< SII1,h,h, .... In) \n",
      "represents the probability of the sentences to be included in \n",
      "the summary based on the given features possessed by the \n",
      "sentence.\n",
      "The probability of classification are learned from the \n",
      "training data by the following Bayes rule [16]: where s rep\u0002resents the set of sentences in the document and fi represents \n",
      "the features used in classification stage and S represents the \n",
      "set of sentences in the summary.\n",
      "SENTENCE LEVEL FEATURES \n",
      "2.1 Sentence location feature \n",
      "The sentences that occur in the beginning and the conclusion \n",
      "part of the document are most likely important since most \n",
      "documents are hierarchically structured with important infor\u0002mation in the beginning and the end of the paragraphs.\n",
      "The sentences are restricted as a non-summary \n",
      "and summary sentence based on the feature possessed by the \n",
      "sentence.\n",
      "The first \n",
      "step involves labeling the training data using a machine\u0002learning approach and then extract features of the sentences \n",
      "in both test set and train sets which is then inputted to the \n",
      "neural network system to rank the sentences in the document.\n",
      "1.2 Title Word feature \n",
      "The sentences in the original document which consists of \n",
      "words mentioned in the title have greater chances to contribute \n",
      "to the final summary since they serve as indicators of the theme \n",
      "of the document.\n",
      "1.5 Upper case word feature \n",
      "The words which are in uppercase such as \"UNICEF\" are \n",
      "considered to be important words and those sentences that \n",
      "consist of these words are termed important in the context of \n",
      "sentence selection for the final summary.\n",
      "The basic steps in concept \n",
      "based summarization are: i) Retrieve concepts of a text from external knowledge base(HowNet, WordNet, Wikipedia) ii) \n",
      "Build a conceptual vector or graph model to depict relationship \n",
      "between concept and sentences iii) Apply ranking algorithm to \n",
      "score sentences iv) Generate summaries based on the ranking \n",
      "scores of sentences \n",
      "4.\n",
      "The major drawback with the \n",
      "supervised approach is that it requires known manually created \n",
      "summaries by a human to label the sentences in the original \n",
      "training document enclosed with \"summary sentence\" or \"non\u0002summary sentence\" and it also requires more labeled training \n",
      "data for classification.\n",
      "The unsupervised approaches do not \n",
      "need human summaries (user input) in deciding the important \n",
      "features of the document, it requires the most sophisticated \n",
      "algorithm to provide compensation for the lack of human \n",
      "knowledge.\n",
      "It is very \n",
      "crucial for humans to understand and to describe the content \n",
      "of the text.\n",
      "LSA captures the text of the \n",
      "input document and excerpt information such as words that \n",
      "frequently occur together and words that are commonly seen in \n",
      "different sentences.\n",
      "In the methodology proposed [12], the \n",
      "importance of sentences is calculated based on the concepts \n",
      "retrieved from HowNet instead of words.\n",
      "The summary is generated by ordering the \n",
      "ranked sentences in the order they occur in the original \n",
      "document to maintain coherency.\n",
      "The sentences in the \n",
      "document are represented as a graph and the edges between \n",
      "the sentences represents weighted cosine similarity values.\n",
      "The pre\u0002processed passage is sent to the feature extraction steps, which \n",
      "is based on multiple features of sentences and words.\n",
      "The significance \n",
      "of sentences is strongly based on statistical and linguistic \n",
      "features of sentences.\n",
      "The \n",
      "scores obtained after the feature extraction are fed to the \n",
      "neural network, which produces a single value as output score, \n",
      "signifying the importance of the sentences.\n",
      "2.4 Sentence-to-Sentence Cohesion \n",
      "The cohesion between sentences for every sentence(s), the \n",
      "similarity between s and alternative sentences are calculated \n",
      "which are summed up and coarse value of the aspect is \n",
      "obtained for s. The feature values are normalized between \n",
      "[0, 1] where value closer to 1.0 indicates a higher degree of \n",
      "cohesion between sentences.\n",
      "The main problem in evaluation comes from \n",
      "the impossibility of building a standard against which the \n",
      "results of the systems that have to be compared.\n",
      "From Fig6 [13] it is to \n",
      "be noted in order that dl is associated to d2 than dO and the \n",
      "conversation 'walked' is linked to the talk 'man' but it is not \n",
      "significant to the word 'park'.\n",
      "In text Summarization, the most challenging task is \n",
      "to summarize the contented from a number of semi\u0002structured sources and textual, which includes web pages \n",
      "and databases, in the proper way (size, format, time, \n",
      "language,) for an explicit user.\n",
      "The approach specified \n",
      "in [20]uses CRF as a sequence labelling problem and also \n",
      "captures interaction between sentences through the features \n",
      "extracted for each sentence and it also incorporates complex \n",
      "features such as LSA_scores [21] and lilTS_score [22] but \n",
      "the limitation is that linguistic features are not considered.\n",
      "The sentence that consists of main keywords is most \n",
      "likely included in the final summary.\n",
      "Another graph based approach \n",
      "LexRank [6], where the salience of the sentence is determined \n",
      "by the concept of Eigen vector centrality.\n",
      "The major phase \n",
      "is the feature fusion phase where the relationship between \n",
      "the features are identified through two stages 1) eliminat\u0002ing infrequent features 2) collapsing frequent features after \n",
      "which sentence ranking is done to identify the important \n",
      "summary sentences.Neural Network [17]after feature fusion \n",
      "is depicted in Fig 8.\n",
      "Ladda Suanmali et al [11] proposed fuzzy logic approach \n",
      "is used for automatic text summarization which is the initial \n",
      "step , the text document is pre-processed followed by feature \n",
      "extraction(Title features, Sentence length, Sentence position, \n",
      "Sentence-sentence similarity, term weight, Proper noun and \n",
      "Numerical data.\n",
      "However, the work has not focused the different \n",
      "challenges of extractive text summarization process to its full \n",
      "intensity in premises of time and space complication.\n",
      "A conceptual vector \n",
      "model is built to obtain a rough summarization and similarity \n",
      "measures are calculated between the sentences to reduce \n",
      "redundancy in the final summary.\n",
      "The proposed system overcomes the issues faced \n",
      "by non-negative matrix Factorization (NMF) methods by in\u0002corporating conditional random fields (CRF) to identify and \n",
      "extract correct features to determine the important sentence \n",
      "of the given text.\n",
      "The \n",
      "sentences are clustered into groups based on their similarity \n",
      "measures and then the sentences are ranked based on their \n",
      "LexRank scores similar to PageRank algorithm [7]except that \n",
      "the similarity graph is undirected in LexRank method.\n",
      "Count(N-gram) is the number of N-grams in the set of \n",
      "reference summaries.\n",
      "The main advantage of the method is that \n",
      "it is able to identify correct features and provides a better \n",
      "representation of sentences and groups terms appropriately \n",
      "into its segments.\n",
      "FEATURES FOR EXTRACTIVE TEXT \n",
      "SUMMARIZATION \n",
      "Earlier techniques involve assigning a score to sentences \n",
      "based on a countenance that are predefined based on the \n",
      "methodology applied.\n",
      "A Higher score is \n",
      "assigned to the paragraph in the peripheral sections (beginning \n",
      "and end paragraphs of the document).\n",
      "Neural network after training (a) and after pruning (b) [17] \n",
      "In the approach proposed in [18], RankNet algorautomati\u0002callyithm using neural nets to identify the important sentences \n",
      "in the document.\n",
      "The main advantage \n",
      "of a text summarization is reading time of the user can be \n",
      "reduced.\n"
     ]
    }
   ],
   "source": [
    "for lines in sentence_list:\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
